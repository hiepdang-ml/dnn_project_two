{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# Dog Heart Vertebral Heart Size Point Detection \n",
    "# 1. Build an object detection model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e55944",
   "metadata": {},
   "source": [
    "First, I create two `Dataset` classes: `LabeledDogHeartDataset` (for labeled data) and `UnlabeledDogHeartDataset` (for unlabeled data). These two classes share some common functionalities. Hence, they inherit from a `BaseDogHearDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict, Literal\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseDogHeartDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataroot: str, \n",
    "        image_resolution: Tuple[int, int], \n",
    "        has_labels: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataroot: str = dataroot\n",
    "        self.image_resolution: Tuple[int, int] = image_resolution\n",
    "        self.image_folder: str = os.path.join(dataroot, 'Images')\n",
    "        self.image_filenames: List[str] = sorted(os.listdir(self.image_folder))\n",
    "        self.has_labels: bool = has_labels\n",
    "        if self.has_labels:\n",
    "            self.point_folder: str = os.path.join(dataroot, 'Labels')\n",
    "            self.point_filenames: List[str] = sorted(os.listdir(self.point_folder))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def transform(self, input: Image.Image) -> torch.Tensor:\n",
    "        transformer = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize(size=self.image_resolution),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transformer(input)\n",
    "\n",
    "\n",
    "class LabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=True)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str, str]:\n",
    "        # Load images and masks\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        point_path: str = os.path.join(self.point_folder, self.point_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        width_original, height_original = image.size\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        height_new, width_new = image_tensor.shape[1], image_tensor.shape[2]\n",
    "        \n",
    "        mat: Dict[Literal['six_points', 'VHS'], np.array] = loadmat(file_name=point_path)\n",
    "        six_points: torch.Tensor = torch.as_tensor(mat['six_points'], dtype=torch.float32)\n",
    "        # Resize image to any size and maintain original points\n",
    "        six_points[:, 0] = width_new / width_original * six_points[:, 0]\n",
    "        six_points[:, 1] = height_new / height_original * six_points[:, 1]\n",
    "        # Normalize\n",
    "        six_points = six_points / height_new\n",
    "\n",
    "        vhs: torch.Tensor = torch.as_tensor(mat['VHS'], dtype=torch.float32).reshape(-1)\n",
    "        return image_tensor, six_points, vhs, image_path, point_path\n",
    "\n",
    "\n",
    "class UnlabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=False)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        # Load images\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        return image_tensor, image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d033d",
   "metadata": {},
   "source": [
    "Create dataset instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db3d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/train', image_resolution=(512, 512))\n",
    "val_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/validation', image_resolution=(512, 512))\n",
    "\n",
    "test_dataset = UnlabeledDogHeartDataset(dataroot='Dog_Heart_VHS/test', image_resolution=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c452a",
   "metadata": {},
   "source": [
    "## Model Architecture:\n",
    "\n",
    "In this project, I built a `Vision Transformer (ViT)` from scratch (https://arxiv.org/abs/2010.11929). This architecrure can be described by the following figure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc408f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Train your model using [Dog VHS Dataset](https://yuad-my.sharepoint.com/:f:/g/personal/youshan_zhang_yu_edu/ErguFJBE4y9KqzEdWWNlXzMBkTbsBaNX9l856SyvQauwJg?e=L3JOuN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50effdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3.Evaluate your model using the test images with the [software](https://github.com/YoushanZhang/Dog-Cardiomegaly_VHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687038bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Your results should be achieved 85%. VHS = 6(AB+CD)/EF\n",
    "\n",
    "## (10 points, accuracy < 75% --> 0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e96710d9",
   "metadata": {},
   "source": [
    "# 5. Show the comprison between predictions and ground truth\n",
    "## You need to add the title with: image name, predicted VHS and Ground Truth VHS\n",
    "<p align=\"center\">\n",
    "  <img src=\"Com.png\" width=\"60%\"> \n",
    "</p>\n",
    "\n",
    "\n",
    "# Please show the comprison results of images: 1420.png, 1479.png and 1530.png from Valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44739b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 6. Write a three-page report using LaTex and upload your paper to ResearchGate or Arxiv, and put your paper link here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c16b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b1a959",
   "metadata": {},
   "source": [
    "# 7. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file, prediction CSV file and add paper link)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeee384",
   "metadata": {},
   "source": [
    "# 8. Bonus points (10 points if your accuracy is higer than 87.3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75c46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

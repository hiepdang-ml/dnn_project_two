{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# Dog Heart Vertebral Heart Size Point Detection \n",
    "# 1. Build an object detection model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e55944",
   "metadata": {},
   "source": [
    "First, I create two `Dataset` classes: `LabeledDogHeartDataset` (for labeled data) and `UnlabeledDogHeartDataset` (for unlabeled data). These two classes share some common functionalities. Hence, they inherit from a `BaseDogHearDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict, Literal\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseDogHeartDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataroot: str, \n",
    "        image_resolution: Tuple[int, int], \n",
    "        has_labels: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataroot: str = dataroot\n",
    "        self.image_resolution: Tuple[int, int] = image_resolution\n",
    "        self.image_folder: str = os.path.join(dataroot, 'Images')\n",
    "        self.image_filenames: List[str] = sorted(os.listdir(self.image_folder))\n",
    "        self.has_labels: bool = has_labels\n",
    "        if self.has_labels:\n",
    "            self.point_folder: str = os.path.join(dataroot, 'Labels')\n",
    "            self.point_filenames: List[str] = sorted(os.listdir(self.point_folder))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def transform(self, input: Image.Image) -> torch.Tensor:\n",
    "        transformer = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize(size=self.image_resolution),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transformer(input)\n",
    "\n",
    "\n",
    "class LabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=True)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str, str]:\n",
    "        # Load images and masks\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        point_path: str = os.path.join(self.point_folder, self.point_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        width_original, height_original = image.size\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        height_new, width_new = image_tensor.shape[1], image_tensor.shape[2]\n",
    "        \n",
    "        mat: Dict[Literal['six_points', 'VHS'], np.array] = loadmat(file_name=point_path)\n",
    "        six_points: torch.Tensor = torch.as_tensor(mat['six_points'], dtype=torch.float32)\n",
    "        # Resize image to any size and maintain original points\n",
    "        six_points[:, 0] = width_new / width_original * six_points[:, 0]\n",
    "        six_points[:, 1] = height_new / height_original * six_points[:, 1]\n",
    "        # Normalize\n",
    "        six_points = six_points / height_new\n",
    "\n",
    "        vhs: torch.Tensor = torch.as_tensor(mat['VHS'], dtype=torch.float32).reshape(-1)\n",
    "        return image_tensor, six_points, vhs, image_path, point_path\n",
    "\n",
    "\n",
    "class UnlabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=False)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        # Load images\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        return image_tensor, image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d033d",
   "metadata": {},
   "source": [
    "Create dataset instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db3d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/train', image_resolution=(512, 512))\n",
    "val_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/validation', image_resolution=(512, 512))\n",
    "\n",
    "test_dataset = UnlabeledDogHeartDataset(dataroot='Dog_Heart_VHS/test', image_resolution=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c452a",
   "metadata": {},
   "source": [
    "## Model Architecture:\n",
    "\n",
    "In this project, I built a `Vision Transformer (ViT)` from scratch (https://arxiv.org/abs/2010.11929). This architecrure can be described by the following figure:\n",
    "\n",
    "<div style=\"background-color:white; width:1000px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/assets/architecture.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2188339",
   "metadata": {},
   "source": [
    "First, I build the `PatchPositionEmbedding` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459debe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PatchPositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        patch_size: int, \n",
    "        embedding_dim: int, \n",
    "        image_size: Tuple[int, int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels: int = in_channels\n",
    "        self.patch_size: int = patch_size\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.image_size: Tuple[int, int] = image_size\n",
    "        self.n_hpatches: int = image_size[0] // patch_size\n",
    "        self.n_wpatches: int = image_size[1] // patch_size\n",
    "        self.n_patches: int = self.n_hpatches * self.n_wpatches\n",
    "        self.projector = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=embedding_dim,\n",
    "            kernel_size=patch_size, stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4  # (batch_size, n_channels, height, width)\n",
    "        batch_size: int = input.shape[0]\n",
    "        output: torch.Tensor = self.projector(input)\n",
    "        assert output.shape == (batch_size, self.embedding_dim, self.n_hpatches, self.n_wpatches)\n",
    "        output: torch.Tensor = output.flatten(start_dim=2, end_dim=-1)\n",
    "        assert output.shape == (batch_size, self.embedding_dim, self.n_patches)\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da46a16",
   "metadata": {},
   "source": [
    "The Transformer Encoder contains a stack of multiple `TransformerBlock`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5091c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int, \n",
    "        n_heads: int, \n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.n_heads: int = n_heads\n",
    "\n",
    "        assert embedding_dim % n_heads == 0, f'embedding_dim must be divisible by n_heads'\n",
    "        self.head_embedding_dim: int = self.embedding_dim // self.n_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(in_features=embedding_dim, out_features=embedding_dim * 3)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim, num_heads=n_heads, \n",
    "            dropout=dropout, batch_first=False,\n",
    "        )\n",
    "        self.projector1 = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.projector2 = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 3\n",
    "        assert input.shape[2] == self.embedding_dim\n",
    "        batch_size: int = input.shape[0]\n",
    "        n_patches: int = input.shape[1]\n",
    "\n",
    "        residual: torch.Tensor = input.clone()\n",
    "        \n",
    "        # LayerNorm\n",
    "        input: torch.Tensor = self.layer_norm1(input)\n",
    "        \n",
    "        # Multihead Attention\n",
    "        qkv: torch.Tensor = self.qkv(input)\n",
    "        assert qkv.shape == (batch_size, n_patches, self.embedding_dim * 3)\n",
    "        qkv: torch.Tensor = qkv.reshape(batch_size, n_patches, 3, self.embedding_dim)\n",
    "        qkv: torch.Tensor = qkv.permute(2, 1, 0, 3)\n",
    "        assert qkv.shape == (3, n_patches, batch_size, self.embedding_dim)\n",
    "        queries: torch.Tensor = qkv[0]\n",
    "        keys: torch.Tensor = qkv[1]\n",
    "        values: torch.Tensor = qkv[2]\n",
    "        output, _ = self.attention(query=queries, key=keys, value=values)\n",
    "        assert output.shape == (n_patches, batch_size, self.embedding_dim)\n",
    "        output: torch.Tensor = output.permute(1, 0, 2)\n",
    "        output = F.gelu(self.projector1(output))\n",
    "\n",
    "        # Residual Connection\n",
    "        output = residual + output\n",
    "        residual: torch.Tensor = output.clone()\n",
    "        # LayerNorm\n",
    "        output = self.layer_norm2(output)\n",
    "        # MLP\n",
    "        output = F.gelu(self.projector2(output))\n",
    "        # Residual Connection\n",
    "        output = residual + output\n",
    "        assert output.shape == (batch_size, n_patches, self.embedding_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fc408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int, \n",
    "        n_heads: int, \n",
    "        depth: int, \n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.n_heads: int = n_heads\n",
    "        self.depth: int = depth\n",
    "        self.dropout: float = dropout\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(embedding_dim, n_heads, dropout) for _ in range(depth)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 3\n",
    "        assert input.shape[2] == self.embedding_dim\n",
    "        batch_size: int = input.shape[0]\n",
    "        n_patches: int = input.shape[1]\n",
    "\n",
    "        output: torch.Tensor = self.blocks(input)\n",
    "        output: torch.Tensor = self.layer_norm(output)\n",
    "        assert output.shape == (batch_size, n_patches, self.embedding_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c589289",
   "metadata": {},
   "source": [
    "We also need an `OrthogonalLayer` to ensure `AB` is perpendicular to `CD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e2d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size: int = input.shape[0]\n",
    "        assert input.shape == (batch_size, 6, 2)\n",
    "        s: torch.Tensor = - (input[:, 0, 0] - input[:, 1, 0]) / (input[:, 0, 1] - input[:, 1, 1])\n",
    "        y3: torch.Tensor = s * (input[:, 3, 0] - input[:, 2, 0]) + input[:, 2, 1]\n",
    "        output = input.clone()\n",
    "        output[:, 3, 1] = y3\n",
    "        assert output.shape == input.shape\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab5680",
   "metadata": {},
   "source": [
    "Lastly, we stack all of the above modules to form the Vision Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8c5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        patch_size: int, \n",
    "        embedding_dim: int, \n",
    "        image_size: Tuple[int, int], \n",
    "        depth: int, \n",
    "        n_heads: int, \n",
    "        dropout: float, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels: int = in_channels\n",
    "        self.out_channels: int = 12\n",
    "        self.patch_size: int = patch_size\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.image_size: Tuple[int, int] = image_size\n",
    "        self.depth: int = depth\n",
    "        self.n_heads: int = n_heads\n",
    "        self.dropout: float = dropout\n",
    "\n",
    "        self.patch_embedding = PatchPositionEmbedding(in_channels, patch_size, embedding_dim, image_size)\n",
    "        self.encoder = TransformerEncoder(embedding_dim, n_heads, depth, dropout)\n",
    "        self.orthogonalizer = OrthogonalLayer()\n",
    "\n",
    "        scale_pos: float = self.patch_embedding.n_patches * embedding_dim\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.rand(1, self.patch_embedding.n_patches, embedding_dim) / scale_pos\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(*[\n",
    "            nn.Linear(in_features=self.patch_embedding.n_patches * embedding_dim, out_features=1024), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=1024, out_features=512), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=512, out_features=self.out_channels),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, n_channels, image_height, image_width = input.shape\n",
    "        output: torch.Tensor = self.patch_embedding(input)\n",
    "        assert output.shape == (batch_size, self.patch_embedding.n_patches, self.embedding_dim)\n",
    "        output: torch.Tensor = output + self.pos_embedding\n",
    "        output: torch.Tensor = self.encoder(output)\n",
    "        assert output.shape == (batch_size, self.patch_embedding.n_patches, self.embedding_dim)\n",
    "        output: torch.Tensor = output.flatten(start_dim=1, end_dim=-1)\n",
    "        output: torch.Tensor = self.mlp_head(output).reshape(batch_size, 6, 2)\n",
    "        output: torch.Tensor = output.reshape(batch_size, 6, 2)\n",
    "        return self.orthogonalizer(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4bfe1",
   "metadata": {},
   "source": [
    "Let's test on random input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be58e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 3, 512, 512])\n",
      "Output shape: torch.Size([8, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "net = VisionTransformer(\n",
    "    in_channels=3, patch_size=32, \n",
    "    embedding_dim=2048, image_size=(512, 512),\n",
    "    depth=12, n_heads=16, dropout=0.,\n",
    ")\n",
    "x = torch.rand(8, 3, 512, 512)\n",
    "y = net(x)\n",
    "\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Output shape: {y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Train your model using [Dog VHS Dataset](https://yuad-my.sharepoint.com/:f:/g/personal/youshan_zhang_yu_edu/ErguFJBE4y9KqzEdWWNlXzMBkTbsBaNX9l856SyvQauwJg?e=L3JOuN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e380d",
   "metadata": {},
   "source": [
    "Before training a `VisionTransformer` model on the `LabeledDogHeartDataset` datasets, we need to define some utility classes to control and monitor the training process. \n",
    "\n",
    "First, we need a `Accumulator` to keep track of the performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddfce527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from typing import Optional, Dict, TextIO, Any, List, Tuple\n",
    "from collections import defaultdict\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"\n",
    "    A utility class for accumulating values for multiple metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__records: defaultdict[str, float] = defaultdict(float)\n",
    "\n",
    "    def add(self, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Add values to the accumulator.\n",
    "\n",
    "        Parameters:\n",
    "            - **kwargs: named metric and the value is the amount to add.\n",
    "        \"\"\"\n",
    "        metric: str\n",
    "        value: float\n",
    "        for metric, value in kwargs.items():\n",
    "            # Each keyword argument represents a metric name and its value to be added\n",
    "            self.__records[metric] += value\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the accumulator by clearing all recorded metrics.\n",
    "        \"\"\"\n",
    "        self.__records.clear()\n",
    "\n",
    "    def __getitem__(self, key: str) -> float:\n",
    "        \"\"\"\n",
    "        Retrieve a record by key.\n",
    "\n",
    "        Parameters:\n",
    "            - key (str): The record key name.\n",
    "\n",
    "        Returns:\n",
    "            - float: The record value.\n",
    "        \"\"\"\n",
    "        return self.__records[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0511d4a",
   "metadata": {},
   "source": [
    "An implementation of `EarlyStopping` mechanism to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b79164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    A simple early stopping utility to terminate training when a monitored metric stops improving.\n",
    "\n",
    "    Attributes:\n",
    "        - patience (int): The number of epochs with no improvement after which training will be stopped.\n",
    "        - tolerance (float): The minimum change in the monitored metric to qualify as an improvement,\n",
    "        - considering the direction of the metric being monitored.\n",
    "        - bestscore (float): The best score seen so far.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int, tolerance: float = 0.) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopping instance.\n",
    "        \n",
    "        Parameters:\n",
    "            - patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "            - tolerance (float): The minimum change in the monitored metric to qualify as an improvement. \n",
    "            Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.patience: int = patience\n",
    "        self.tolerance: float = tolerance\n",
    "        self.bestscore: float = float('inf')\n",
    "        self.__counter: int = 0\n",
    "\n",
    "    def __call__(self, value: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the state of the early stopping mechanism based on the new metric value.\n",
    "\n",
    "        Parameters:\n",
    "            - value (float): The latest value of the monitored metric.\n",
    "        \"\"\"\n",
    "        # Improvement or within tolerance, reset counter\n",
    "        if value <= self.bestscore + self.tolerance:\n",
    "            self.bestscore: float = value\n",
    "            self.__counter: int = 0\n",
    "\n",
    "        # No improvement, increment counter\n",
    "        else:\n",
    "            self.__counter += 1\n",
    "\n",
    "    def __bool__(self) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the training process should be stopped early.\n",
    "\n",
    "        Returns:\n",
    "            - bool: True if training should be stopped (patience exceeded), otherwise False.\n",
    "        \"\"\"\n",
    "        return self.__counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c2f25",
   "metadata": {},
   "source": [
    "A `Timer` to monitor the running time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0423e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "\n",
    "    \"\"\"\n",
    "    A class used to time the duration of epochs and batches.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Timer.\n",
    "        \"\"\"\n",
    "        self.__epoch_starts: Dict[int, float] = dict()\n",
    "        self.__epoch_ends: Dict[int, float] = dict()\n",
    "        self.__batch_starts: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "        self.__batch_ends: Dict[int, Dict[int, float]] = defaultdict(dict)\n",
    "\n",
    "    def start_epoch(self, epoch: int) -> None:\n",
    "        \"\"\"\n",
    "        Start timing an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            epoch (int): The epoch number.\n",
    "        \"\"\"\n",
    "        self.__epoch_starts[epoch] = time.time()\n",
    "\n",
    "    def end_epoch(self, epoch: int) -> None:\n",
    "        \"\"\"\n",
    "        End timing an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "        \"\"\"\n",
    "        self.__epoch_ends[epoch] = time.time()\n",
    "\n",
    "    def start_batch(self, epoch: int, batch: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Start timing a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int, optional): The batch number. If not provided, the next batch number is used.\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            if self.__batch_starts[epoch]:\n",
    "                batch: int = max(self.__batch_starts[epoch].keys()) + 1\n",
    "            else:\n",
    "                batch: int = 1\n",
    "        self.__batch_starts[epoch][batch] = time.time()\n",
    "    \n",
    "    def end_batch(self, epoch: int, batch: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        End timing a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int, optional): The batch number. If not provided, the last started batch number is used.\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            if self.__batch_starts[epoch]:\n",
    "                batch: int = max(self.__batch_starts[epoch].keys())\n",
    "            else:\n",
    "                raise RuntimeError(f\"no batch has started\")\n",
    "        self.__batch_ends[epoch][batch] = time.time()\n",
    "    \n",
    "    def time_epoch(self, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the duration of an epoch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "\n",
    "        Returns:\n",
    "            - float: The duration of the epoch in seconds.\n",
    "        \"\"\"\n",
    "        result: float = self.__epoch_ends[epoch] - self.__epoch_starts[epoch]\n",
    "        if result > 0:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError(f\"epoch {epoch} ends before starts\")\n",
    "    \n",
    "    def time_batch(self, epoch: int, batch: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the duration of a batch.\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The epoch number.\n",
    "            - batch (int): The batch number.\n",
    "\n",
    "        Returns:\n",
    "            - float: The duration of the batch in seconds.\n",
    "        \"\"\"\n",
    "        result: float = self.__batch_ends[epoch][batch] - self.__batch_starts[epoch][batch]\n",
    "        if result > 0:\n",
    "            return result\n",
    "        else:\n",
    "            raise RuntimeError(f\"batch {batch} in epoch {epoch} ends before starts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1138386",
   "metadata": {},
   "source": [
    "A `Logger` to log the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f750b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    \"\"\"\n",
    "    A class used to log the training process.\n",
    "\n",
    "    This class provides methods to log messages to a file and the console. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        logfile: str = f\".logs/{dt.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    ) -> None:\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize the logger.\n",
    "\n",
    "        Parameters:\n",
    "            - logfile (str, optional): The path to the logfile. \n",
    "            Defaults to a file in the .logs directory with the current timestamp.\n",
    "        \"\"\"\n",
    "        self.logfile: pathlib.Path = pathlib.Path(logfile)\n",
    "        os.makedirs(name=self.logfile.parent, exist_ok=True)\n",
    "        self._file: TextIO = open(self.logfile, mode='w')\n",
    "\n",
    "    def log(\n",
    "        self, \n",
    "        epoch: int, \n",
    "        n_epochs: int, \n",
    "        batch: Optional[int] = None, \n",
    "        n_batches: Optional[int] = None, \n",
    "        took: Optional[float] = None, \n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to console and a log file\n",
    "\n",
    "        Parameters:\n",
    "            - epoch (int): The current epoch.\n",
    "            - n_epochs (int): The total number of epochs.\n",
    "            - batch (int, optional): The current batch. Defaults to None.\n",
    "            - n_batches (int, optional): The total number of batches. Defaults to None.\n",
    "            - took (float, optional): The time it took to process the batch or epoch. Defaults to None.\n",
    "            - **kwargs: Additional metrics to log.\n",
    "        \"\"\"\n",
    "        suffix: str = ', '.join([f'{metric}: {value:.3e}' for metric, value in kwargs.items()])\n",
    "        prefix: str = f'Epoch {epoch}/{n_epochs} | '\n",
    "        if batch is not None:\n",
    "            prefix += f'Batch {batch}/{n_batches} | '\n",
    "        if took is not None:\n",
    "            prefix += f'Took {took:.2f}s | '\n",
    "        logstring: str = prefix + suffix\n",
    "        print(logstring)\n",
    "        self._file.write(logstring + '\\n')\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the logfile at garbage collected.\n",
    "        \"\"\"\n",
    "        self._file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88122586",
   "metadata": {},
   "source": [
    "A `CheckPointSaver` to ragularly save to model checkpoint during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a3f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckPointSaver:\n",
    "    \"\"\"\n",
    "    A class used to save PyTorch model checkpoints.\n",
    "\n",
    "    Attributes:\n",
    "        - dirpath (pathlib.Path): The directory where the checkpoints are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dirpath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CheckPointSaver.\n",
    "\n",
    "        Parameters:\n",
    "            - dirpath (os.PathLike): The directory where the checkpoints are saved.\n",
    "        \"\"\"\n",
    "        self.dirpath: pathlib.Path = pathlib.Path(dirpath)\n",
    "        os.makedirs(name=self.dirpath, exist_ok=True)\n",
    "\n",
    "    def save(self, model: nn.Module, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save checkpoint to a .pt file.\n",
    "\n",
    "        Parameters:\n",
    "            - model (nn.Module): The PyTorch model to save.\n",
    "            - filename (str): the checkpoint file name\n",
    "        \"\"\"\n",
    "        torch.save(obj=model, f=os.path.join(self.dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2ab27",
   "metadata": {},
   "source": [
    "A `compute_vhs` function to compute the VHS on a batch of 6 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc8fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_vhs(points: torch.Tensor) -> torch.Tensor:\n",
    "    assert points.shape[1:] == (6, 2), 'Each sample in points should be in shape (6, 2)'\n",
    "    batch_size: int = points.shape[0]\n",
    "    AB = torch.norm(points[:, 1] - points[:, 0], dim=1)\n",
    "    CD = torch.norm(points[:, 3] - points[:, 2], dim=1)\n",
    "    EF = torch.norm(points[:, 5] - points[:, 4], dim=1)\n",
    "    vhs = 6 * (AB + CD) / EF\n",
    "    return vhs.reshape(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba24e7",
   "metadata": {},
   "source": [
    "A `plot_predictions` to plot the predicted points against the groundtruth points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50effdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    image_path: str, \n",
    "    gt_points: Optional[torch.Tensor], \n",
    "    pred_points: torch.Tensor,\n",
    "):\n",
    "    assert pred_points.shape == (6, 2), 'points should be in shape (6, 2)'\n",
    "    # Make sure all tensors are in CPU\n",
    "    pred_points = pred_points.to(device='cpu')\n",
    "    if gt_points is not None:\n",
    "        assert gt_points.shape == (6, 2), 'points should be in shape (6, 2)'\n",
    "        gt_points = gt_points.to(device='cpu')\n",
    "\n",
    "    # Load image\n",
    "    image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Scale points\n",
    "    pred_points = pred_points * torch.tensor(image.size, dtype=pred_points.dtype)\n",
    "    if gt_points is not None:\n",
    "        gt_points = gt_points * torch.tensor(image.size, dtype=gt_points.dtype)\n",
    "\n",
    "    # Draw points    \n",
    "    plt.scatter(x=pred_points[:, 0], y=pred_points[:, 1], color='red', label='Prediction')\n",
    "    if gt_points is not None:\n",
    "        plt.scatter(x=gt_points[:, 0], y=gt_points[:, 1], color='green', label='Groundtruth')\n",
    "\n",
    "    # Draw lines\n",
    "    for p1, p2 in [(0, 1), (2, 3), (4, 5)]:\n",
    "        plt.plot(\n",
    "            [pred_points[p1, 0], pred_points[p2, 0]], [pred_points[p1, 1], pred_points[p2, 1]], \n",
    "            color='r', linestyle='--',\n",
    "        )\n",
    "        if gt_points is not None:\n",
    "            plt.plot(\n",
    "                [gt_points[p1, 0], gt_points[p2, 0]], [gt_points[p1, 1], gt_points[p2, 1]], \n",
    "                color='g', linestyle='-'\n",
    "            )\n",
    "\n",
    "    # Report the VHS in figure title\n",
    "    filename: str = os.path.basename(image_path)\n",
    "    title = f'{filename}\\n'\n",
    "    if gt_points is not None:\n",
    "        title += f'Groundtruth VHS: {compute_vhs(gt_points.unsqueeze(0)).item():.4f}, '\n",
    "\n",
    "    title += f'Predicted VHS: {compute_vhs(pred_points.unsqueeze(0)).item():.4f}'\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set legend\n",
    "    plt.legend(loc='upper right')\n",
    "    # Fit plot margins\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.05, top=0.9)\n",
    "    # Save file\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    plt.savefig(f'results/{filename}')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af538d",
   "metadata": {},
   "source": [
    "With all the necessary utilities, now we can define a `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba6ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Optimizer, Adam\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        optimizer: Optimizer,\n",
    "        train_batch_size: int,\n",
    "        val_batch_size: int,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device=device)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.optimizer = optimizer\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(dataset=val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        n_epochs: int,\n",
    "        patience: int,\n",
    "        tolerance: float,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \n",
    "        train_metrics = Accumulator()\n",
    "        early_stopping = EarlyStopping(patience, tolerance)\n",
    "        timer = Timer()\n",
    "        logger = Logger()\n",
    "        checkpoint_saver = CheckPointSaver(dirpath=checkpoint_path)\n",
    "        self.model.train()\n",
    "\n",
    "        # loop through each epoch\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            timer.start_epoch(epoch)\n",
    "            # Loop through each batch\n",
    "            for batch, (batch_images, batch_sixpoints, *_) in enumerate(self.train_dataloader, start=1):\n",
    "                timer.start_batch(epoch, batch)\n",
    "                batch_images: torch.Tensor = batch_images.to(device=self.device)\n",
    "                batch_sixpoints: torch.Tensor = batch_sixpoints.to(device=self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                pred_targets: torch.Tensor = self.model(input=batch_images)\n",
    "                print(f'gt_targets {batch_sixpoints[-1]}')\n",
    "                print(f'pred_targets {pred_targets[-1]}')\n",
    "                mse_loss = self.loss_function(input=pred_targets, target=batch_sixpoints)\n",
    "                mse_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Accumulate the metrics\n",
    "                train_metrics.add(mse_loss=mse_loss.item())\n",
    "                timer.end_batch(epoch=epoch)\n",
    "                logger.log(\n",
    "                    epoch=epoch, n_epochs=n_epochs, \n",
    "                    batch=batch, n_batches=len(self.train_dataloader), \n",
    "                    took=timer.time_batch(epoch, batch), \n",
    "                    train_mse_loss=train_metrics['mse_loss'] / batch, \n",
    "                )\n",
    "\n",
    "            # Ragularly save checkpoint\n",
    "            if checkpoint_path and epoch % 5 == 0:\n",
    "                checkpoint_saver.save(self.model, filename=f'epoch{epoch}.pt')\n",
    "            \n",
    "            # Reset metric records for next epoch\n",
    "            train_metrics.reset()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_mse_loss = self.evaluate()\n",
    "            timer.end_epoch(epoch)\n",
    "            logger.log(\n",
    "                epoch=epoch, n_epochs=n_epochs, \n",
    "                took=timer.time_epoch(epoch), \n",
    "                val_mse_loss=val_mse_loss,\n",
    "            )\n",
    "            print('=' * 20)\n",
    "\n",
    "            early_stopping(val_mse_loss)\n",
    "            if early_stopping:\n",
    "                print('Early Stopped')\n",
    "                break\n",
    "\n",
    "        # Save last checkpoint\n",
    "        if checkpoint_path:\n",
    "            checkpoint_saver.save(self.model, filename=f'epoch{epoch}.pt')\n",
    "\n",
    "    def evaluate(self) -> float:\n",
    "        val_metrics = Accumulator()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Loop through each batch\n",
    "            for batch, (batch_images, batch_sixpoints, *_) in enumerate(self.val_dataloader, start=1):\n",
    "                batch_images: torch.Tensor = batch_images.to(device=self.device)\n",
    "                batch_sixpoints: torch.Tensor = batch_sixpoints.to(device=self.device)\n",
    "                pred_targets: torch.Tensor = self.model(input=batch_images)\n",
    "                mse_loss = self.loss_function(input=pred_targets, target=batch_sixpoints)\n",
    "                # Accumulate the val_metrics\n",
    "                val_metrics.add(mse_loss=mse_loss.item())\n",
    "\n",
    "        # Compute the aggregate metrics\n",
    "        return val_metrics['mse_loss'] / batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f076b",
   "metadata": {},
   "source": [
    "We also need a `Predictor` to make six-point predictions on a trained model against any dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfda8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "\n",
    "    def __init__(self, model: nn.Module, device: torch.device) -> None:\n",
    "        self.model: nn.Module = model.to(device=device)\n",
    "        self.device: torch.device = device\n",
    "\n",
    "    def predict(self, dataset: Dataset, need_plots: bool) -> pd.DataFrame:\n",
    "        self.model.eval()\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        image_paths: List[str] = []\n",
    "        point_predictions: List[torch.Tensor] = []\n",
    "        vhs_predictions: List[torch.Tensor] = []\n",
    "\n",
    "        if isinstance(dataloader.dataset, UnlabeledDogHeartDataset):\n",
    "            with torch.no_grad():\n",
    "                # Loop through each batch\n",
    "                for batch_images, batch_image_paths in dataloader:\n",
    "                    batch_images: torch.Tensor = batch_images.to(device=self.device)\n",
    "                    pred_points: torch.Tensor = self.model(input=batch_images)\n",
    "                    pred_vhs: torch.Tensor = compute_vhs(points=pred_points)\n",
    "\n",
    "                    image_paths.extend(batch_image_paths)\n",
    "                    point_predictions.append(pred_points)\n",
    "                    vhs_predictions.append(pred_vhs)\n",
    "\n",
    "                point_predictions = torch.cat(tensors=point_predictions, dim=0).to(device=self.device)\n",
    "                vhs_predictions: torch.Tensor = torch.cat(tensors=vhs_predictions, dim=0).reshape(-1).to(device=self.device)\n",
    "                if need_plots:\n",
    "                    assert point_predictions.shape[0] == len(image_paths)\n",
    "                    for i in range(len(image_paths)):\n",
    "                        image_path: str = image_paths[i]\n",
    "                        point_prediction: torch.Tensor = point_predictions[i]\n",
    "                        plot_predictions(\n",
    "                            image_path=image_path, gt_points=None, pred_points=point_prediction,\n",
    "                        )\n",
    "\n",
    "        elif isinstance(dataloader.dataset, LabeledDogHeartDataset):\n",
    "            point_groundtruths: List[torch.Tensor] = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Loop through each batch\n",
    "                for batch_images, batch_gt_six_points, _, batch_image_paths, _ in dataloader:\n",
    "                    batch_images: torch.Tensor = batch_images.to(device=self.device)\n",
    "                    batch_gt_six_points: torch.Tensor = batch_gt_six_points.to(device=self.device)\n",
    "                    pred_points: torch.Tensor = self.model(input=batch_images)\n",
    "                    pred_vhs: torch.Tensor = compute_vhs(points=pred_points)\n",
    "\n",
    "                    image_paths.extend(batch_image_paths)\n",
    "                    point_predictions.append(pred_points)\n",
    "                    vhs_predictions.append(pred_vhs)\n",
    "                    point_groundtruths.append(batch_gt_six_points)\n",
    "\n",
    "                vhs_predictions: torch.Tensor = torch.cat(tensors=vhs_predictions, dim=0).reshape(-1).to(device=self.device)\n",
    "                point_predictions = torch.cat(tensors=point_predictions, dim=0).to(device=self.device)\n",
    "                point_groundtruths = torch.cat(tensors=point_groundtruths, dim=0).to(device=self.device)\n",
    "                if need_plots:\n",
    "                    assert (\n",
    "                        point_predictions.shape[0] \n",
    "                        == point_groundtruths.shape[0] \n",
    "                        == vhs_predictions.shape[0]\n",
    "                        == len(image_paths) \n",
    "                    )\n",
    "                    for i in range(len(image_paths)):\n",
    "                        image_path: str = image_paths[i]\n",
    "                        point_prediction: torch.Tensor = point_predictions[i]\n",
    "                        point_groundtruth: torch.Tensor = point_groundtruths[i]\n",
    "                        plot_predictions(\n",
    "                            image_path=image_path, gt_points=point_groundtruth, pred_points=point_prediction,\n",
    "                        )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid dataset')\n",
    "\n",
    "        prediction_table = pd.DataFrame(\n",
    "            data={\n",
    "                'image': [os.path.basename(image_path) for image_path in image_paths], \n",
    "                'label': vhs_predictions.cpu().numpy().tolist(),\n",
    "            }\n",
    "        )\n",
    "        prediction_table.to_csv(\n",
    "            f'{dt.datetime.now().strftime(r\"%Y%m%d%H%M%S\")}.csv', \n",
    "            header=False, \n",
    "            index=False\n",
    "        )\n",
    "        return prediction_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e6e46",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00faac",
   "metadata": {},
   "source": [
    "Since I `build and train a large-scale ViT model from scratch`, I had to `ssh` into a GPU cloud instance which hosts 2 `RTX A6000` GPUs, each has `48GB` of VRAM. The training process required parallelism and synchronous data transfer between these two GPUs, effectively leveraging vertical scaling. The training batch size is set `128` which is large enough for parallel processing across both GPUs while compensating the overhead of data transfer.\n",
    "\n",
    "Although vertical scaling on a multi-GPU cloud instance is efficient, `it is not manageable in a Jupyter notebook environment`, which is intended only for experimental prototyping.\n",
    "\n",
    "While the cell below was not directly run in this notebook, the resulting model’s `checkpoint` can be found here: https://drive.google.com/file/d/12moljBHBecJP5S0T2bi7NCAVr09IT524/view?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aab49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device('cuda')\n",
    "learning_rate: float = 1e-7\n",
    "\n",
    "net = VisionTransformer(\n",
    "    in_channels=3, patch_size=32, \n",
    "    embedding_dim=4096, image_size=(512, 512),\n",
    "    depth=16, n_heads=32, dropout=0.,\n",
    ")\n",
    "\n",
    "net = nn.DataParallel(module=net).to(device=device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=net, \n",
    "    train_dataset=train_dataset, val_dataset=val_dataset, \n",
    "    optimizer=Adam(params=net.parameters(), lr=learning_rate),\n",
    "    train_batch_size=128, val_batch_size=32,\n",
    "    device=device,\n",
    ")\n",
    "trainer.train(\n",
    "    n_epochs=1000, \n",
    "    patience=100, tolerance=0., \n",
    "    checkpoint_path='.checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974945e",
   "metadata": {},
   "source": [
    "From the trained model, one might run the inference script as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e798b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device: torch.device = torch.device('cuda')\n",
    "predictor = Predictor(model=net, device=device)\n",
    "\n",
    "predictor.predict(dataset=test_dataset, need_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3.Evaluate your model using the test images with the [software](https://github.com/YoushanZhang/Dog-Cardiomegaly_VHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd835f0",
   "metadata": {},
   "source": [
    "The predicting script above generates a `.csv` file in a format that is expected by the evaluation software. You should be able to verify the result below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f7049",
   "metadata": {},
   "source": [
    "<div style=\"background-color:white; width:700px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/assets/predictions.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a0fd7",
   "metadata": {},
   "source": [
    "The `.csv` can also be downloaded at: https://github.com/hiepdang-ml/dnn_project_two/blob/master/20240723209958.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Your results should be achieved 85%. VHS = 6(AB+CD)/EF\n",
    "\n",
    "## (10 points, accuracy < 75% --> 0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60095d3",
   "metadata": {},
   "source": [
    "The `EarlyStopping` triggered the training process to stop at epoch `480`. So far, the ViT model was only able to achieve `83%` accuracy. \n",
    "\n",
    "Further improvement is required in later work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96710d9",
   "metadata": {},
   "source": [
    "# 5. Show the comprison between predictions and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b65de",
   "metadata": {},
   "source": [
    "### `1420.png`\n",
    "\n",
    "<div style=\"background-color:white; width:700px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/results/1420.png\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### `1479.png`\n",
    "\n",
    "<div style=\"background-color:white; width:700px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/results/1479.png\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### `1530.png`\n",
    "\n",
    "<div style=\"background-color:white; width:700px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/results/1530.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 6. Write a three-page report using LaTex and upload your paper to ResearchGate or Arxiv, and put your paper link here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640fdd2",
   "metadata": {},
   "source": [
    "Paper link: https://www.researchgate.net/publication/382491046_Vision_Transformer-Based_Approach_for_Accurate_Cardiomegaly_Detection_in_Canines\n",
    "\n",
    "Source code: https://github.com/hiepdang-ml/dnn_project_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1a959",
   "metadata": {},
   "source": [
    "# 7. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file, prediction CSV file and add paper link)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeee384",
   "metadata": {},
   "source": [
    "# 8. Bonus points (10 points if your accuracy is higer than 87.3%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe2065",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

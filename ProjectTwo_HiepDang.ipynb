{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# Dog Heart Vertebral Heart Size Point Detection \n",
    "# 1. Build an object detection model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e55944",
   "metadata": {},
   "source": [
    "First, I create two `Dataset` classes: `LabeledDogHeartDataset` (for labeled data) and `UnlabeledDogHeartDataset` (for unlabeled data). These two classes share some common functionalities. Hence, they inherit from a `BaseDogHearDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict, Literal\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BaseDogHeartDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataroot: str, \n",
    "        image_resolution: Tuple[int, int], \n",
    "        has_labels: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataroot: str = dataroot\n",
    "        self.image_resolution: Tuple[int, int] = image_resolution\n",
    "        self.image_folder: str = os.path.join(dataroot, 'Images')\n",
    "        self.image_filenames: List[str] = sorted(os.listdir(self.image_folder))\n",
    "        self.has_labels: bool = has_labels\n",
    "        if self.has_labels:\n",
    "            self.point_folder: str = os.path.join(dataroot, 'Labels')\n",
    "            self.point_filenames: List[str] = sorted(os.listdir(self.point_folder))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def transform(self, input: Image.Image) -> torch.Tensor:\n",
    "        transformer = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize(size=self.image_resolution),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transformer(input)\n",
    "\n",
    "\n",
    "class LabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=True)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str, str]:\n",
    "        # Load images and masks\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        point_path: str = os.path.join(self.point_folder, self.point_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        width_original, height_original = image.size\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        height_new, width_new = image_tensor.shape[1], image_tensor.shape[2]\n",
    "        \n",
    "        mat: Dict[Literal['six_points', 'VHS'], np.array] = loadmat(file_name=point_path)\n",
    "        six_points: torch.Tensor = torch.as_tensor(mat['six_points'], dtype=torch.float32)\n",
    "        # Resize image to any size and maintain original points\n",
    "        six_points[:, 0] = width_new / width_original * six_points[:, 0]\n",
    "        six_points[:, 1] = height_new / height_original * six_points[:, 1]\n",
    "        # Normalize\n",
    "        six_points = six_points / height_new\n",
    "\n",
    "        vhs: torch.Tensor = torch.as_tensor(mat['VHS'], dtype=torch.float32).reshape(-1)\n",
    "        return image_tensor, six_points, vhs, image_path, point_path\n",
    "\n",
    "\n",
    "class UnlabeledDogHeartDataset(BaseDogHeartDataset):\n",
    "\n",
    "    def __init__(self, dataroot: str, image_resolution: Tuple[int, int]):\n",
    "        super().__init__(dataroot, image_resolution, has_labels=False)\n",
    "\n",
    "    # implement\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, str]:\n",
    "        # Load images\n",
    "        image_path: str = os.path.join(self.image_folder, self.image_filenames[idx])\n",
    "        image: Image.Image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor: torch.Tensor = self.transform(input=image)\n",
    "        return image_tensor, image_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d033d",
   "metadata": {},
   "source": [
    "Create dataset instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db3d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/train', image_resolution=(512, 512))\n",
    "val_dataset = LabeledDogHeartDataset(dataroot='Dog_Heart_VHS/validation', image_resolution=(512, 512))\n",
    "\n",
    "test_dataset = UnlabeledDogHeartDataset(dataroot='Dog_Heart_VHS/test', image_resolution=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c452a",
   "metadata": {},
   "source": [
    "## Model Architecture:\n",
    "\n",
    "In this project, I built a `Vision Transformer (ViT)` from scratch (https://arxiv.org/abs/2010.11929). This architecrure can be described by the following figure:\n",
    "\n",
    "<div style=\"background-color:white; width:1000px\">\n",
    "    <img src=\"https://raw.githubusercontent.com/hiepdang-ml/dnn_project_two/master/assets/architecture.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2188339",
   "metadata": {},
   "source": [
    "First, I build the `PatchPositionEmbedding` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459debe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PatchPositionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        patch_size: int, \n",
    "        embedding_dim: int, \n",
    "        image_size: Tuple[int, int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels: int = in_channels\n",
    "        self.patch_size: int = patch_size\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.image_size: Tuple[int, int] = image_size\n",
    "        self.n_hpatches: int = image_size[0] // patch_size\n",
    "        self.n_wpatches: int = image_size[1] // patch_size\n",
    "        self.n_patches: int = self.n_hpatches * self.n_wpatches\n",
    "        self.projector = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=embedding_dim,\n",
    "            kernel_size=patch_size, stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4  # (batch_size, n_channels, height, width)\n",
    "        batch_size: int = input.shape[0]\n",
    "        output: torch.Tensor = self.projector(input)\n",
    "        assert output.shape == (batch_size, self.embedding_dim, self.n_hpatches, self.n_wpatches)\n",
    "        output: torch.Tensor = output.flatten(start_dim=2, end_dim=-1)\n",
    "        assert output.shape == (batch_size, self.embedding_dim, self.n_patches)\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da46a16",
   "metadata": {},
   "source": [
    "The Transformer Encoder contains a stack of multiple `TransformerBlock`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5091c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int, \n",
    "        n_heads: int, \n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.n_heads: int = n_heads\n",
    "\n",
    "        assert embedding_dim % n_heads == 0, f'embedding_dim must be divisible by n_heads'\n",
    "        self.head_embedding_dim: int = self.embedding_dim // self.n_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(in_features=embedding_dim, out_features=embedding_dim * 3)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim, num_heads=n_heads, \n",
    "            dropout=dropout, batch_first=False,\n",
    "        )\n",
    "        self.projector1 = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.projector2 = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 3\n",
    "        assert input.shape[2] == self.embedding_dim\n",
    "        batch_size: int = input.shape[0]\n",
    "        n_patches: int = input.shape[1]\n",
    "\n",
    "        residual: torch.Tensor = input.clone()\n",
    "        \n",
    "        # LayerNorm\n",
    "        input: torch.Tensor = self.layer_norm1(input)\n",
    "        \n",
    "        # Multihead Attention\n",
    "        qkv: torch.Tensor = self.qkv(input)\n",
    "        assert qkv.shape == (batch_size, n_patches, self.embedding_dim * 3)\n",
    "        qkv: torch.Tensor = qkv.reshape(batch_size, n_patches, 3, self.embedding_dim)\n",
    "        qkv: torch.Tensor = qkv.permute(2, 1, 0, 3)\n",
    "        assert qkv.shape == (3, n_patches, batch_size, self.embedding_dim)\n",
    "        queries: torch.Tensor = qkv[0]\n",
    "        keys: torch.Tensor = qkv[1]\n",
    "        values: torch.Tensor = qkv[2]\n",
    "        output, _ = self.attention(query=queries, key=keys, value=values)\n",
    "        assert output.shape == (n_patches, batch_size, self.embedding_dim)\n",
    "        output: torch.Tensor = output.permute(1, 0, 2)\n",
    "        output = F.gelu(self.projector1(output))\n",
    "\n",
    "        # Residual Connection\n",
    "        output = residual + output\n",
    "        residual: torch.Tensor = output.clone()\n",
    "        # LayerNorm\n",
    "        output = self.layer_norm2(output)\n",
    "        # MLP\n",
    "        output = F.gelu(self.projector2(output))\n",
    "        # Residual Connection\n",
    "        output = residual + output\n",
    "        assert output.shape == (batch_size, n_patches, self.embedding_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fc408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int, \n",
    "        n_heads: int, \n",
    "        depth: int, \n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.n_heads: int = n_heads\n",
    "        self.depth: int = depth\n",
    "        self.dropout: float = dropout\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(embedding_dim, n_heads, dropout) for _ in range(depth)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 3\n",
    "        assert input.shape[2] == self.embedding_dim\n",
    "        batch_size: int = input.shape[0]\n",
    "        n_patches: int = input.shape[1]\n",
    "\n",
    "        output: torch.Tensor = self.blocks(input)\n",
    "        output: torch.Tensor = self.layer_norm(output)\n",
    "        assert output.shape == (batch_size, n_patches, self.embedding_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c589289",
   "metadata": {},
   "source": [
    "We also need an `OrthogonalLayer` to ensure `AB` is perpendicular to `CD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e2d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size: int = input.shape[0]\n",
    "        assert input.shape == (batch_size, 6, 2)\n",
    "        s: torch.Tensor = - (input[:, 0, 0] - input[:, 1, 0]) / (input[:, 0, 1] - input[:, 1, 1])\n",
    "        y3: torch.Tensor = s * (input[:, 3, 0] - input[:, 2, 0]) + input[:, 2, 1]\n",
    "        output = input.clone()\n",
    "        output[:, 3, 1] = y3\n",
    "        assert output.shape == input.shape\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab5680",
   "metadata": {},
   "source": [
    "Lastly, we stack all of the above modules to form the Vision Transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8c5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        patch_size: int, \n",
    "        embedding_dim: int, \n",
    "        image_size: Tuple[int, int], \n",
    "        depth: int, \n",
    "        n_heads: int, \n",
    "        dropout: float, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels: int = in_channels\n",
    "        self.out_channels: int = 12\n",
    "        self.patch_size: int = patch_size\n",
    "        self.embedding_dim: int = embedding_dim\n",
    "        self.image_size: Tuple[int, int] = image_size\n",
    "        self.depth: int = depth\n",
    "        self.n_heads: int = n_heads\n",
    "        self.dropout: float = dropout\n",
    "\n",
    "        self.patch_embedding = PatchPositionEmbedding(in_channels, patch_size, embedding_dim, image_size)\n",
    "        self.encoder = TransformerEncoder(embedding_dim, n_heads, depth, dropout)\n",
    "        self.orthogonalizer = OrthogonalLayer()\n",
    "\n",
    "        scale_pos: float = self.patch_embedding.n_patches * embedding_dim\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.rand(1, self.patch_embedding.n_patches, embedding_dim) / scale_pos\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(*[\n",
    "            nn.Linear(in_features=self.patch_embedding.n_patches * embedding_dim, out_features=1024), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=1024, out_features=512), nn.ReLU(), nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=512, out_features=self.out_channels),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        assert input.ndim == 4\n",
    "        batch_size, n_channels, image_height, image_width = input.shape\n",
    "        output: torch.Tensor = self.patch_embedding(input)\n",
    "        assert output.shape == (batch_size, self.patch_embedding.n_patches, self.embedding_dim)\n",
    "        output: torch.Tensor = output + self.pos_embedding\n",
    "        output: torch.Tensor = self.encoder(output)\n",
    "        assert output.shape == (batch_size, self.patch_embedding.n_patches, self.embedding_dim)\n",
    "        output: torch.Tensor = output.flatten(start_dim=1, end_dim=-1)\n",
    "        output: torch.Tensor = self.mlp_head(output).reshape(batch_size, 6, 2)\n",
    "        output: torch.Tensor = output.reshape(batch_size, 6, 2)\n",
    "        return self.orthogonalizer(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4bfe1",
   "metadata": {},
   "source": [
    "Let's test on random input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be58e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 512, 512])\n",
      "torch.Size([8, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "net = VisionTransformer(\n",
    "    in_channels=1, patch_size=32, embedding_dim=256,\n",
    "    image_size=(512, 512), depth=2, n_heads=16, dropout=0.1,\n",
    ")\n",
    "x = torch.rand(8, 1, 512, 512)\n",
    "y = net(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Train your model using [Dog VHS Dataset](https://yuad-my.sharepoint.com/:f:/g/personal/youshan_zhang_yu_edu/ErguFJBE4y9KqzEdWWNlXzMBkTbsBaNX9l856SyvQauwJg?e=L3JOuN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50effdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3.Evaluate your model using the test images with the [software](https://github.com/YoushanZhang/Dog-Cardiomegaly_VHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687038bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Your results should be achieved 85%. VHS = 6(AB+CD)/EF\n",
    "\n",
    "## (10 points, accuracy < 75% --> 0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a626823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e96710d9",
   "metadata": {},
   "source": [
    "# 5. Show the comprison between predictions and ground truth\n",
    "## You need to add the title with: image name, predicted VHS and Ground Truth VHS\n",
    "<p align=\"center\">\n",
    "  <img src=\"Com.png\" width=\"60%\"> \n",
    "</p>\n",
    "\n",
    "\n",
    "# Please show the comprison results of images: 1420.png, 1479.png and 1530.png from Valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44739b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 6. Write a three-page report using LaTex and upload your paper to ResearchGate or Arxiv, and put your paper link here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c16b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56b1a959",
   "metadata": {},
   "source": [
    "# 7. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file, prediction CSV file and add paper link)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeee384",
   "metadata": {},
   "source": [
    "# 8. Bonus points (10 points if your accuracy is higer than 87.3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75c46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
